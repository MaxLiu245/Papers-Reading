# 2 梯度爆炸与梯度消失问题

> tags:
>
> #Math Concepts

最近准备读一些有关于ODE和NN的文章，并看到了一些有关可逆ResNet的描述，并搜集了一些资料发现，相关的方法似乎常常会遇到梯度计算上的问题，主要包括：

1. 参数量可能较大，如ODE-Net，原文说明的减少内存消耗的说法还没使我信服

2. 训练网络的时候似乎有很多trick，因为搜到了不少有关“how to train ...”的文章

3. ResNet以残差的方式使梯度不容易消失

基于以上背景，我决定重新了解一下有关梯度爆炸和梯度消失的概念，不求掌握多种解决方法，先把概念搞明白

---

查阅了相关文献后我大概明白了，这两个问题总体上都是由于网络训练中使用链式法则进行梯度的传播导致的

以一个单隐层为例，假设输入为 $\textbf{x}$，输出为 $\textbf{y}$，网络的线性层权重参数为 $\textbf{w}$，且激活函数为 $f$，则该单隐层表示为：

$$\displaystyle \textbf{y} = f(\textbf{w}^T \textbf{x})$$

所以计算网络参数的梯度得到：

$$\displaystyle \dfrac{\partial \textbf{y}}{\partial \textbf{x}} = \textbf{w} \dfrac{\partial f(\textbf{w}^T \textbf{x})}{\partial \textbf{w}^T \textbf{x}}$$

上式最后面的第一项就是权重参数 $\textbf{w}$，第二项则是激活函数 $f$ 的导数。因此下一步迭代时参数的更新**既与权重参数本身的值有关，又与激活函数的导数性质有关**。当二者的矩阵乘积分解为 $A^T B A$ 的形式，其中 $B$ 是对角元素为特征值的对角阵时，若特征值绝对值都大于1，则梯度的效果是**放大**，反之效果是**收缩**。进一步，多隐层NN的多层之间梯度通过链式法则层层累积，就容易出现不确定的情形，当梯度累积得很大，梯度就**炸**了，很小就**消失**。

这样基本的概念就了解了。现在可以联想到某些应用，比如调参的时候往往说参数（$w$）的初始化不要太大，要小一些，这就有防止梯度在刚开始训练的时候就爆炸的效果；而不同激活函数的选择也对此现象有所影响，比如sigmoid函数的梯度是始终不大于1的，相对来说爆炸的可能性就笑了一些，而ReLu的梯度是为1或者0的，这样激活函数的影响就没有原来那么大，梯度是爆炸还是消失就要看网络参数 $w$ 了。这样分析，其实不同的激活函数只能缓解梯度的爆炸或消失的问题，目前个人还没有去查阅是否有设计好的激活函数能更好地减少这些情况或者直接避免（应该没有...有的话估计被吹爆...）

---
参考链接：

* 大部分笔记的来源：知乎问答[为什么会有梯度爆炸和梯度消失？](https://www.zhihu.com/question/290392414)
* 知乎文章：[RNN梯度消失和爆炸的原因](https://zhuanlan.zhihu.com/p/28687529)
* ziyubiti的博客[深度神经网络的梯度不稳定问题--梯度消失与梯度爆炸](https://ziyubiti.github.io/2016/11/06/gradvanish/)

